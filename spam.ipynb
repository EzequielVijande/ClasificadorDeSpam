{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clasificador de spam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este notebook contiene la implementacion en python de un clasificador Bayesiano para corres electronicos.El clasificador distinguie dos categorias, si un correo electronico es 'spam' (contenido indeseado o irrelevante para el usuario) o no."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descarga de base de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-03-14 11:56:17--  https://raw.githubusercontent.com/rn-2019-itba/Clase-2--Hiperparametros-y-Tecnicas-de-Validacion/master/Opcional/data/emails.csv\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.216.133\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.216.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 8954755 (8,5M) [text/plain]\n",
      "Saving to: ‘emails.csv’\n",
      "\n",
      "emails.csv          100%[===================>]   8,54M   344KB/s    in 26s     \n",
      "\n",
      "2020-03-14 11:56:43 (342 KB/s) - ‘emails.csv’ saved [8954755/8954755]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! wget 'https://raw.githubusercontent.com/rn-2019-itba/Clase-2--Hiperparametros-y-Tecnicas-de-Validacion/master/Opcional/data/emails.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizacion de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       Subject: naturally irresistible your corporate...\n",
      "1       Subject: the stock trading gunslinger  fanny i...\n",
      "2       Subject: unbelievable new homes made easy  im ...\n",
      "3       Subject: 4 color printing special  request add...\n",
      "4       Subject: do not have money , get software cds ...\n",
      "                              ...                        \n",
      "5723    Subject: re : research and development charges...\n",
      "5724    Subject: re : receipts from visit  jim ,  than...\n",
      "5725    Subject: re : enron case study update  wow ! a...\n",
      "5726    Subject: re : interest  david ,  please , call...\n",
      "5727    Subject: news : aurora 5 . 2 update  aurora ve...\n",
      "Name: text, Length: 5728, dtype: object\n",
      "Subject: naturally irresistible your corporate identity  lt is really hard to recollect a company : the  market is full of suqgestions and the information isoverwhelminq ; but a good  catchy logo , stylish statlonery and outstanding website  will make the task much easier .  we do not promise that havinq ordered a iogo your  company will automaticaily become a world ieader : it isguite ciear that  without good products , effective business organization and practicable aim it  will be hotat nowadays market ; but we do promise that your marketing efforts  will become much more effective . here is the list of clear  benefits : creativeness : hand - made , original logos , specially done  to reflect your distinctive company image . convenience : logo and stationery  are provided in all formats ; easy - to - use content management system letsyou  change your website content and even its structure . promptness : you  will see logo drafts within three business days . affordability : your  marketing break - through shouldn ' t make gaps in your budget . 100 % satisfaction  guaranteed : we provide unlimited amount of changes with no extra fees for you to  be surethat you will love the result of this collaboration . have a look at our  portfolio _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ not interested . . . _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "0       1\n",
      "1       1\n",
      "2       1\n",
      "3       1\n",
      "4       1\n",
      "       ..\n",
      "5723    0\n",
      "5724    0\n",
      "5725    0\n",
      "5726    0\n",
      "5727    0\n",
      "Name: spam, Length: 5728, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as p\n",
    "\n",
    "file_path = \"emails.csv\"\n",
    "emails = p.read_csv(file_path)\n",
    "print(emails['text'])\n",
    "print(emails['text'][0])\n",
    "print(emails['spam'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocesamiento y descarga de datos filtrados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizado, Lemming, eliminacion de stop words y de tokens que no son palabras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/nutria/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/nutria/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/nutria/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesados: 0\n",
      "Procesados: 1000\n",
      "Procesados: 2000\n",
      "Procesados: 3000\n",
      "Procesados: 4000\n",
      "Procesados: 5000\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "emails_filtrados = list()\n",
    "for i in range( len(emails['text']) ):\n",
    "    if (i %1000) == 0:\n",
    "        print(\"Procesados: \" + str(i))\n",
    "    #Tokenizado\n",
    "    tokens = word_tokenize(emails['text'][0])\n",
    "    #Lemming\n",
    "    lem= [lemmatizer.lemmatize(x,pos='v') for x in tokens]\n",
    "    #Stop words\n",
    "    stop = [x for x in lem if x not in stopwords.words('english')]\n",
    "    #Stemming\n",
    "    stem=[stemmer.stem(x) for x in stop]\n",
    "    #Remove non-words\n",
    "    alpha=[x for x in stem if x.isalpha()]\n",
    "    #Agrego el nuevo email filtrado a la lista\n",
    "    emails_filtrados.append( \" \".join(alpha) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Guardo los mails conseguidos hasta esta etapa de pre-procesamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('em_filt.pck', 'wb') as fp:\n",
    "    pickle.dump(emails_filtrados, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
